{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c641cfc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du crawling pour : https://www.snopes.com/\n",
      "Contenu sauvegardé dans : crawled_content\\join.txt\n",
      "Contenu sauvegardé dans : crawled_content\\top.txt\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/politics/?pagenum=0: 404\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/Entertainment/?pagenum=0: 404\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checktitanic-jp-morgan.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkblack-waitress-fired-musk.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkgaetz-ag-teens-post.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkai-tiktok-polar-bear-cub-rescue.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkdemocrats-osama-bin-laden-pic.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkmusk-officially-buying-mcdonalds.txt\n",
      "Contenu sauvegardé dans : crawled_content\\news20241107trump-project-2025-plan.txt\n",
      "Contenu sauvegardé dans : crawled_content\\news20241106trump-win-2024-presidential-election.txt\n",
      "Contenu sauvegardé dans : crawled_content\\news20241105harris-faked-phone-call.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionshalloween-legends-rumors.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionstrump-2024-presidential-election.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionsharris-walz-quotes-election.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionsharris-walz-images.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionstrump-vance-misleading-photos.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionsharris-walz-misleading-videos.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionstrump-vance-misleading-videos.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionswalz-election-claims.txt\n",
      "Contenu sauvegardé dans : crawled_content\\about.txt\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/entertainment/?pagenum=0: 404\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/science/?pagenum=0: 404\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/lifestyle/?pagenum=0: 404\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkisrael-greta-thunberg-sign.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checklana-del-rey-lizzo-feud.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkelf-2-on-max.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checklos-angeles-underground-starbucks.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checktrump-diddy-good-guy.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checktrump-arrested-scam-text.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkgiant-turtle-video.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkvampire-killing-kit-19th-century.txt\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/history/?pagenum=0: 404\n",
      "Contenu sauvegardé dans : crawled_content\\articles383824frederick-valentich-ufo-disappearance.txt\n",
      "Contenu sauvegardé dans : crawled_content\\faqs.txt\n",
      "Contenu sauvegardé dans : crawled_content\\random.txt\n",
      "Démarrage du crawling pour : https://www.factcheck.org/\n",
      "Contenu sauvegardé dans : crawled_content\\feed.txt\n",
      "Contenu sauvegardé dans : crawled_content\\the-factcheck-wirepage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\scicheck.txt\n",
      "Contenu sauvegardé dans : crawled_content\\tag2024-electionspage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fake-newspage2.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fake-newspage115.txt\n",
      "Contenu sauvegardé dans : crawled_content\\persondonald-trumppage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issuehealthpage2.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issuehealthpage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issuehealthpage4.txt\n",
      "Contenu sauvegardé dans : crawled_content\\a-guide-to-our-coronavirus-coverage.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issueclimate-change.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issueclimate-changepage2.txt\n",
      "Contenu sauvegardé dans : crawled_content\\archives.txt\n",
      "Contenu sauvegardé dans : crawled_content\\hot-topics.txt\n",
      "Contenu sauvegardé dans : crawled_content\\on-the-airpage2.txt\n",
      "Contenu sauvegardé dans : crawled_content\\on-the-airpage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411the-2024-factcheck-awards.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411harris-vs-trump-on-climate-change.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202410donald-trumps-closing-arguments.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411musk-did-not-ban-stephen-king-from-x-contrary-to-online-claims.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411posts-falsely-claim-cbs-news-reported-cheating-in-election.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411raskin-didnt-say-he-wont-be-certifying-the-election.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411trump-makes-unsupported-claim-about-massive-cheating-in-philadelphia.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411posts-spread-unfounded-claim-of-race-based-threat-of-violence-in-georgia.txt\n",
      "Contenu sauvegardé dans : crawled_content\\page3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\newsfeed-defenders.txt\n",
      "Démarrage du crawling pour : https://factcheck.afp.com/\n",
      "Le crawl est interdit pour https://factcheck.afp.com selon robots.txt.\n",
      "Démarrage du crawling pour : https://beforeitsnews.com/\n",
      "Le crawl est interdit pour https://beforeitsnews.com selon robots.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import urllib.robotparser\n",
    "from requests.exceptions import RequestException\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements spécifiques de BeautifulSoup\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "\n",
    "# Fonction pour normaliser les URLs (enlever les fragments et les barres obliques finales)\n",
    "def normalize_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    normalized = parsed._replace(fragment=\"\").geturl()\n",
    "    return normalized.rstrip('/')\n",
    "\n",
    "# Fonction pour créer un nom de fichier valide à partir d'une URL\n",
    "def url_to_filename(url):\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.replace(\"/\", \"\").strip(\" \")\n",
    "    if not path:\n",
    "        path = \"index\"  # Nom par défaut pour les URLs racines\n",
    "    return path + \".txt\"\n",
    "\n",
    "# Fonction pour sauvegarder le contenu dans un fichier\n",
    "def save_content_to_file(url, content):\n",
    "    output_dir = \"crawled_content\"  # Répertoire pour sauvegarder les fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Créer le répertoire s'il n'existe pas\n",
    "    filename = url_to_filename(url)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Contenu sauvegardé dans : {filepath}\")\n",
    "\n",
    "# Vérifier les règles du fichier robots.txt pour une URL donnée\n",
    "def can_crawl(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    \n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(\"*\", url)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier robots.txt pour {robots_url}: {e}\")\n",
    "        return True  # En cas d'erreur, autoriser le crawl par défaut\n",
    "\n",
    "# Fonction pour crawler et récupérer le contenu\n",
    "def crawl_and_save_content(url, visited=set(), file_limit=501, file_count=[0], keywords=None):\n",
    "    if file_count[0] >= file_limit:\n",
    "        return  # Arrêter si la limite de fichiers est atteinte\n",
    "\n",
    "    normalized_url = normalize_url(url)\n",
    "    if normalized_url in visited:\n",
    "        return  # Ne pas revisiter les URLs déjà visitées\n",
    "\n",
    "    visited.add(normalized_url)  # Marquer l'URL normalisée comme visitée\n",
    "\n",
    "    # Vérifier si l'URL peut être crawled\n",
    "    if not can_crawl(normalized_url):\n",
    "        print(f\"Le crawl est interdit pour {normalized_url} selon robots.txt.\")\n",
    "        return\n",
    "\n",
    "    # Faire la requête HTTP\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(normalized_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Vérification des CAPTCHA\n",
    "            if \"captcha\" in response.text.lower():\n",
    "                print(f\"Page protégée par CAPTCHA détectée : {normalized_url}\")\n",
    "                return  # Ignorer cette page et continuer avec le crawl\n",
    "\n",
    "            # Utilisation uniquement du parser HTML pour éviter les avertissements\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extraire tout le texte de la page\n",
    "            text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Vérification des mots-clés\n",
    "            if text_content and any(keyword in text_content for keyword in keywords):\n",
    "                save_content_to_file(normalized_url, text_content)\n",
    "                file_count[0] += 1  # Incrémenter le compteur de fichiers\n",
    "\n",
    "            # Extraire et suivre tous les liens internes\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if file_count[0] >= file_limit:\n",
    "                    break  # Arrêter si la limite de fichiers est atteinte\n",
    "                full_url = urljoin(normalized_url, link['href'])  # Résoudre les liens relatifs\n",
    "                \n",
    "                # Vérifier si le lien est interne\n",
    "                if full_url.startswith(url):\n",
    "                    crawl_and_save_content(full_url, visited, file_limit, file_count, keywords)\n",
    "        else:\n",
    "            print(f\"Erreur lors de l'accès à {normalized_url}: {response.status_code}\")\n",
    "    except RequestException as e:\n",
    "        print(f\"Échec de la requête pour {normalized_url}: {e}\")\n",
    "\n",
    "# Liste des URLs de départ\n",
    "starting_urls = [\n",
    "    \"https://www.snopes.com/\",\n",
    "    \"https://www.factcheck.org/\",\n",
    "    \"https://factcheck.afp.com/\",\n",
    "    \"https://beforeitsnews.com/\"\n",
    "]\n",
    "\n",
    "# Liste des mots-clés à rechercher\n",
    "keywords = [\"fake news\", \"hoax\", \"conspiracy theory\"]\n",
    "\n",
    "# Crawler chaque site de départ\n",
    "for url in starting_urls:\n",
    "    print(f\"Démarrage du crawling pour : {url}\")\n",
    "    crawl_and_save_content(url, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f774fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Fonction pour normaliser les URLs (enlever les fragments et les barres obliques finales)\n",
    "def normalize_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    normalized = parsed._replace(fragment=\"\").geturl()\n",
    "    return normalized.rstrip('/')\n",
    "\n",
    "# Fonction pour créer un nom de fichier valide à partir d'une URL\n",
    "def url_to_filename(url):\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.replace(\"/\", \"\").strip(\" \")\n",
    "    if not path:\n",
    "        path = \"index\"  # Nom par défaut pour les URLs racines\n",
    "    return path + \".txt\"\n",
    "\n",
    "# Fonction pour sauvegarder le contenu dans un fichier\n",
    "def save_content_to_file(url, content):\n",
    "    output_dir = \"crawled\"  # Répertoire pour sauvegarder les fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Créer le répertoire s'il n'existe pas\n",
    "    filename = url_to_filename(url)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Contenu sauvegardé dans : {filepath}\")\n",
    "\n",
    "# Fonction pour crawler et récupérer le contenu\n",
    "def crawl_and_save_content(url, visited=set(), file_limit=501, file_count=[0], keywords=None):\n",
    "    if file_count[0] >= file_limit:\n",
    "        return  # Arrêter si la limite de fichiers est atteinte\n",
    "\n",
    "    normalized_url = normalize_url(url)\n",
    "    if normalized_url in visited:\n",
    "        return  # Ne pas revisiter les URLs déjà visitées\n",
    "\n",
    "    visited.add(normalized_url)  # Marquer l'URL normalisée comme visitée\n",
    "\n",
    "    # Faire la requête HTTP\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(normalized_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Vérification des CAPTCHA\n",
    "            if \"captcha\" in response.text.lower():\n",
    "                print(f\"Page protégée par CAPTCHA détectée : {normalized_url}\")\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extraire tout le texte de la page\n",
    "            text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Vérification des mots-clés\n",
    "            if text_content and any(keyword in text_content for keyword in keywords):\n",
    "                save_content_to_file(normalized_url, text_content)\n",
    "                file_count[0] += 1  # Incrémenter le compteur de fichiers\n",
    "\n",
    "            # Extraire et suivre tous les liens internes\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if file_count[0] >= file_limit:\n",
    "                    break  # Arrêter si la limite de fichiers est atteinte\n",
    "                full_url = urljoin(normalized_url, link['href'])  # Résoudre les liens relatifs\n",
    "                \n",
    "                # Vérifier si le lien est interne\n",
    "                if full_url.startswith(url):\n",
    "                    crawl_and_save_content(full_url, visited, file_limit, file_count, keywords)\n",
    "        else:\n",
    "            print(f\"Erreur lors de l'accès à {normalized_url}: {response.status_code}\")\n",
    "    except RequestException as e:\n",
    "        print(f\"Échec de la requête pour {normalized_url}: {e}\")\n",
    "\n",
    "# Liste des URLs de départ\n",
    "starting_urls =[\"https://www.hoaxbuster.com/\",\"https://www.lemonde.fr/les-decodeurs/\",\"https://factuel.afp.com/\",\"https://www.liberation.fr/checknews/\"]\n",
    "\n",
    "# Liste des mots-clés à rechercher\n",
    "keywords = [\"fausse information\", \"désinformation\", \"théorie du complot\"]\n",
    "\n",
    "# Crawler chaque site de départ\n",
    "for url in starting_urls:\n",
    "    print(f\"Démarrage du crawling pour : {url}\")\n",
    "    crawl_and_save_content(url, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617820d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b517a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
