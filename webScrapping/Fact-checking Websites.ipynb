{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c641cfc3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du crawling pour : https://www.snopes.com/\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/politics/?pagenum=0: 404\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/Entertainment/?pagenum=0: 404\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checktitanic-jp-morgan.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkgaetz-ag-teens-post.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionshalloween-legends-rumors.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionstrump-2024-presidential-election.txt\n",
      "Contenu sauvegardé dans : crawled_content\\collectionswalz-election-claims.txt\n",
      "Contenu sauvegardé dans : crawled_content\\about.txt\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/entertainment/?pagenum=0: 404\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/science/?pagenum=0: 404\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/lifestyle/?pagenum=0: 404\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checkelf-2-on-max.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checklos-angeles-underground-starbucks.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checktrump-arrested-scam-text.txt\n",
      "Erreur lors de l'accès à https://www.snopes.com/category/history/?pagenum=0: 404\n",
      "Contenu sauvegardé dans : crawled_content\\articles383824frederick-valentich-ufo-disappearance.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fact-checktourist-roller-coaster.txt\n",
      "Contenu sauvegardé dans : crawled_content\\faqs.txt\n",
      "Démarrage du crawling pour : https://www.factcheck.org/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASSUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\html\\parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu sauvegardé dans : crawled_content\\feed.txt\n",
      "Contenu sauvegardé dans : crawled_content\\scicheck.txt\n",
      "Contenu sauvegardé dans : crawled_content\\fake-newspage115.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issuehealthpage2.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issuehealthpage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\a-guide-to-our-coronavirus-coverage.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issueclimate-change.txt\n",
      "Contenu sauvegardé dans : crawled_content\\issueclimate-changepage2.txt\n",
      "Contenu sauvegardé dans : crawled_content\\archives.txt\n",
      "Contenu sauvegardé dans : crawled_content\\hot-topics.txt\n",
      "Contenu sauvegardé dans : crawled_content\\on-the-airpage3.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411harris-vs-trump-on-climate-change.txt\n",
      "Contenu sauvegardé dans : crawled_content\\202411musks-starlink-was-not-connected-to-vote-tabulation-contrary-to-online-claims.txt\n",
      "Démarrage du crawling pour : https://factcheck.afp.com/\n",
      "Le crawl est interdit pour https://factcheck.afp.com selon robots.txt.\n",
      "Démarrage du crawling pour : https://beforeitsnews.com/\n",
      "Le crawl est interdit pour https://beforeitsnews.com selon robots.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import urllib.robotparser\n",
    "from requests.exceptions import RequestException\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements spécifiques de BeautifulSoup\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"bs4\")\n",
    "\n",
    "# Fonction pour normaliser les URLs (enlever les fragments et les barres obliques finales)\n",
    "def normalize_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    normalized = parsed._replace(fragment=\"\").geturl()\n",
    "    return normalized.rstrip('/')\n",
    "\n",
    "# Fonction pour créer un nom de fichier valide à partir d'une URL\n",
    "def url_to_filename(url):\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.replace(\"/\", \"\").strip(\" \")\n",
    "    if not path:\n",
    "        path = \"index\"  # Nom par défaut pour les URLs racines\n",
    "    return path + \".txt\"\n",
    "\n",
    "# Fonction pour sauvegarder le contenu dans un fichier\n",
    "def save_content_to_file(url, content):\n",
    "    output_dir = \"crawled_content\"  # Répertoire pour sauvegarder les fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Créer le répertoire s'il n'existe pas\n",
    "    filename = url_to_filename(url)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Contenu sauvegardé dans : {filepath}\")\n",
    "\n",
    "# Vérifier les règles du fichier robots.txt pour une URL donnée\n",
    "def can_crawl(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    \n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    try:\n",
    "        rp.set_url(robots_url)\n",
    "        rp.read()\n",
    "        return rp.can_fetch(\"*\", url)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier robots.txt pour {robots_url}: {e}\")\n",
    "        return True  # En cas d'erreur, autoriser le crawl par défaut\n",
    "\n",
    "# Fonction pour crawler et récupérer le contenu\n",
    "def crawl_and_save_content(url, visited=set(), file_limit=501, file_count=[0], keywords=None):\n",
    "    if file_count[0] >= file_limit:\n",
    "        return  # Arrêter si la limite de fichiers est atteinte\n",
    "\n",
    "    normalized_url = normalize_url(url)\n",
    "    if normalized_url in visited:\n",
    "        return  # Ne pas revisiter les URLs déjà visitées\n",
    "\n",
    "    visited.add(normalized_url)  # Marquer l'URL normalisée comme visitée\n",
    "\n",
    "    # Vérifier si l'URL peut être crawled\n",
    "    if not can_crawl(normalized_url):\n",
    "        print(f\"Le crawl est interdit pour {normalized_url} selon robots.txt.\")\n",
    "        return\n",
    "\n",
    "    # Faire la requête HTTP\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(normalized_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Vérification des CAPTCHA\n",
    "            if \"captcha\" in response.text.lower():\n",
    "                print(f\"Page protégée par CAPTCHA détectée : {normalized_url}\")\n",
    "                return  # Ignorer cette page et continuer avec le crawl\n",
    "\n",
    "            # Utilisation uniquement du parser HTML pour éviter les avertissements\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extraire tout le texte de la page\n",
    "            text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Vérification des mots-clés\n",
    "            if text_content and any(keyword in text_content for keyword in keywords):\n",
    "                save_content_to_file(normalized_url, text_content)\n",
    "                file_count[0] += 1  # Incrémenter le compteur de fichiers\n",
    "\n",
    "            # Extraire et suivre tous les liens internes\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if file_count[0] >= file_limit:\n",
    "                    break  # Arrêter si la limite de fichiers est atteinte\n",
    "                full_url = urljoin(normalized_url, link['href'])  # Résoudre les liens relatifs\n",
    "                \n",
    "                # Vérifier si le lien est interne\n",
    "                if full_url.startswith(url):\n",
    "                    crawl_and_save_content(full_url, visited, file_limit, file_count, keywords)\n",
    "        else:\n",
    "            print(f\"Erreur lors de l'accès à {normalized_url}: {response.status_code}\")\n",
    "    except RequestException as e:\n",
    "        print(f\"Échec de la requête pour {normalized_url}: {e}\")\n",
    "\n",
    "# Liste des URLs de départ\n",
    "starting_urls = [\n",
    "    \"https://www.snopes.com/\",\n",
    "    \"https://www.factcheck.org/\",\n",
    "    \"https://factcheck.afp.com/\",\n",
    "    \"https://beforeitsnews.com/\"\n",
    "]\n",
    "\n",
    "# Liste des mots-clés à rechercher\n",
    "keywords = [\"fake news\", \"hoax\", \"conspiracy theory\"]\n",
    "\n",
    "# Crawler chaque site de départ\n",
    "for url in starting_urls:\n",
    "    print(f\"Démarrage du crawling pour : {url}\")\n",
    "    crawl_and_save_content(url, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f774fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "# Fonction pour normaliser les URLs (enlever les fragments et les barres obliques finales)\n",
    "def normalize_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    normalized = parsed._replace(fragment=\"\").geturl()\n",
    "    return normalized.rstrip('/')\n",
    "\n",
    "# Fonction pour créer un nom de fichier valide à partir d'une URL\n",
    "def url_to_filename(url):\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.replace(\"/\", \"\").strip(\" \")\n",
    "    if not path:\n",
    "        path = \"index\"  # Nom par défaut pour les URLs racines\n",
    "    return path + \".txt\"\n",
    "\n",
    "# Fonction pour sauvegarder le contenu dans un fichier\n",
    "def save_content_to_file(url, content):\n",
    "    output_dir = \"crawled\"  # Répertoire pour sauvegarder les fichiers\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Créer le répertoire s'il n'existe pas\n",
    "    filename = url_to_filename(url)\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"Contenu sauvegardé dans : {filepath}\")\n",
    "\n",
    "# Fonction pour crawler et récupérer le contenu\n",
    "def crawl_and_save_content(url, visited=set(), file_limit=501, file_count=[0], keywords=None):\n",
    "    if file_count[0] >= file_limit:\n",
    "        return  # Arrêter si la limite de fichiers est atteinte\n",
    "\n",
    "    normalized_url = normalize_url(url)\n",
    "    if normalized_url in visited:\n",
    "        return  # Ne pas revisiter les URLs déjà visitées\n",
    "\n",
    "    visited.add(normalized_url)  # Marquer l'URL normalisée comme visitée\n",
    "\n",
    "    # Faire la requête HTTP\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(normalized_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Vérification des CAPTCHA\n",
    "            if \"captcha\" in response.text.lower():\n",
    "                print(f\"Page protégée par CAPTCHA détectée : {normalized_url}\")\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extraire tout le texte de la page\n",
    "            text_content = soup.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            # Vérification des mots-clés\n",
    "            if text_content and any(keyword in text_content for keyword in keywords):\n",
    "                save_content_to_file(normalized_url, text_content)\n",
    "                file_count[0] += 1  # Incrémenter le compteur de fichiers\n",
    "\n",
    "            # Extraire et suivre tous les liens internes\n",
    "            links = soup.find_all('a', href=True)\n",
    "            for link in links:\n",
    "                if file_count[0] >= file_limit:\n",
    "                    break  # Arrêter si la limite de fichiers est atteinte\n",
    "                full_url = urljoin(normalized_url, link['href'])  # Résoudre les liens relatifs\n",
    "                \n",
    "                # Vérifier si le lien est interne\n",
    "                if full_url.startswith(url):\n",
    "                    crawl_and_save_content(full_url, visited, file_limit, file_count, keywords)\n",
    "        else:\n",
    "            print(f\"Erreur lors de l'accès à {normalized_url}: {response.status_code}\")\n",
    "    except RequestException as e:\n",
    "        print(f\"Échec de la requête pour {normalized_url}: {e}\")\n",
    "\n",
    "# Liste des URLs de départ\n",
    "starting_urls =[\"https://www.hoaxbuster.com/\",\"https://www.lemonde.fr/les-decodeurs/\",\"https://factuel.afp.com/\",\"https://www.liberation.fr/checknews/\"]\n",
    "\n",
    "# Liste des mots-clés à rechercher\n",
    "keywords = [\"fausse information\", \"désinformation\", \"théorie du complot\"]\n",
    "\n",
    "# Crawler chaque site de départ\n",
    "for url in starting_urls:\n",
    "    print(f\"Démarrage du crawling pour : {url}\")\n",
    "    crawl_and_save_content(url, keywords=keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617820d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b517a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
